{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP001\\AppData\\Local\\Temp\\ipykernel_14116\\4025571736.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data =pd.read_csv(\"Data/Twitter/08-05-2023_06-00-44.csv\", sep=\",\", engine=\"python\", error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "data =pd.read_csv(\"Data/Twitter/08-05-2023_06-00-44.csv\", sep=\",\", engine=\"python\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        Information hand\n",
       "1                               Totally enjoying app ü•∞ü•∞‚ù§Ô∏è\n",
       "2                                  Thanks For Twitter App\n",
       "3       Very awesome though early muh detailed analysi...\n",
       "4                                                Nice app\n",
       "                              ...                        \n",
       "9995           So much better since acquisition late 2022\n",
       "9996                                         Twitter good\n",
       "9997    Terrible app Tweets dissappear youre reading m...\n",
       "9998    I hate lastest version Its divided following üöÆ...\n",
       "9999                              Dont waste life twitter\n",
       "Name: content, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lda(preprocessed_reviews, num_topics):\n",
    "    # Create a dictionary from the preprocessed reviews\n",
    "    dictionary = corpora.Dictionary(preprocessed_reviews)\n",
    "\n",
    "    # Create a bag-of-words (BoW) representation of the reviews\n",
    "    bow_corpus = [dictionary.doc2bow(review) for review in preprocessed_reviews]\n",
    "\n",
    "    # Apply LDA\n",
    "    lda_model = models.LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=50)\n",
    "\n",
    "    # Print the topics\n",
    "    for idx, topic in lda_model.print_topics():\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(review):\n",
    "    doc = nlp(review)\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Using cached clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Using cached emoji-1.7.0.tar.gz (175 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ftfy<7.0,>=6.0\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\hp001\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.6)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171075 sha256=b8d955c608f156a8680fa22d48ba1b8cfd3bae9f9b03677a700225cb382a2d75\n",
      "  Stored in directory: c:\\users\\hp001\\appdata\\local\\pip\\cache\\wheels\\31\\8a\\8c\\315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, ftfy, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\HP001\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews_list(reviews):\n",
    "    # Convert to lowercase\n",
    "    for (i,k) in enumerate(reviews):\n",
    "        for (j,l) in enumerate(k):\n",
    "            reviews[i][j]=clean(l.lower(),no_emoji=True)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))       \n",
    "    filtered_reviews=[[word for word in token if word not in stop_words] for token in reviews]   \n",
    "   # reviews = [review.lower() for review in reviews]\n",
    "\n",
    "    # Tokenization\n",
    "    #tokenized_reviews = [word_tokenize(review) for review in reviews]\n",
    "\n",
    "    # Remove stopwords\n",
    "   # stop_words = set(stopwords.words('english'))\n",
    "   # filtered_reviews = [[word for word in tokens if word not in stop_words] for tokens in tokenized_reviews]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preprocessed_reviews = [[lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_reviews]\n",
    "\n",
    "    return preprocessed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nouns= [get_noun(str(i)) for i in data[\"content\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data=preprocess_reviews_list(data_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.148*\"medium\" + 0.117*\"thing\" + 0.098*\"change\" + 0.037*\"life\" + 0.036*\"friend\" + 0.035*\"picture\" + 0.031*\"source\" + 0.028*\"truth\" + 0.025*\"news\" + 0.025*\"system\"\n",
      "Topic 1: 0.133*\"post\" + 0.105*\"bug\" + 0.058*\"one\" + 0.054*\"fix\" + 0.041*\"view\" + 0.033*\"error\" + 0.030*\"mind\" + 0.029*\"load\" + 0.029*\"playing\" + 0.028*\"date\"\n",
      "Topic 2: 0.128*\"use\" + 0.106*\"thanks\" + 0.082*\"screen\" + 0.078*\"fun\" + 0.073*\"elon\" + 0.064*\"control\" + 0.050*\"follower\" + 0.045*\"world\" + 0.024*\"bar\" + 0.023*\"response\"\n",
      "Topic 3: 0.126*\"star\" + 0.094*\"phone\" + 0.082*\"button\" + 0.073*\"timeline\" + 0.066*\"number\" + 0.066*\"play\" + 0.049*\"need\" + 0.048*\"u\" + 0.017*\"thank\" + 0.015*\"developer\"\n",
      "Topic 4: 0.298*\"time\" + 0.088*\"issue\" + 0.064*\"trash\" + 0.048*\"guy\" + 0.042*\"tap\" + 0.033*\"connection\" + 0.030*\"playback\" + 0.015*\"waste\" + 0.015*\"feature\" + 0.014*\"load\"\n",
      "Topic 5: 0.837*\"app\" + 0.014*\"world\" + 0.010*\"code\" + 0.008*\"country\" + 0.007*\"term\" + 0.004*\"channel\" + 0.003*\"default\" + 0.003*\"product\" + 0.003*\"ratio\" + 0.003*\"lag\"\n",
      "Topic 6: 0.222*\"experience\" + 0.126*\"love\" + 0.059*\"message\" + 0.049*\"improvement\" + 0.042*\"others\" + 0.033*\"owner\" + 0.028*\"person\" + 0.026*\"voice\" + 0.023*\"party\" + 0.017*\"authentication\"\n",
      "Topic 7: 0.206*\"problem\" + 0.123*\"year\" + 0.070*\"porn\" + 0.062*\"rule\" + 0.045*\"mess\" + 0.032*\"propaganda\" + 0.032*\"network\" + 0.029*\"list\" + 0.027*\"section\" + 0.018*\"download\"\n",
      "Topic 8: 0.125*\"feed\" + 0.116*\"apps\" + 0.078*\"ad\" + 0.073*\"internet\" + 0.050*\"photo\" + 0.040*\"data\" + 0.040*\"good\" + 0.027*\"idea\" + 0.026*\"start\" + 0.018*\"tap\"\n",
      "Topic 9: 0.077*\"money\" + 0.058*\"space\" + 0.049*\"today\" + 0.047*\"help\" + 0.039*\"group\" + 0.033*\"entertainment\" + 0.032*\"website\" + 0.031*\"bit\" + 0.018*\"joke\" + 0.018*\"conversation\"\n",
      "Topic 10: 0.220*\"platform\" + 0.062*\"garbage\" + 0.052*\"team\" + 0.052*\"medium\" + 0.044*\"support\" + 0.041*\"comment\" + 0.034*\"quality\" + 0.031*\"site\" + 0.022*\"check\" + 0.022*\"point\"\n",
      "Topic 11: 0.115*\"work\" + 0.084*\"reason\" + 0.068*\"week\" + 0.054*\"reply\" + 0.038*\"policy\" + 0.033*\"part\" + 0.030*\"tab\" + 0.030*\"device\" + 0.025*\"limit\" + 0.024*\"interface\"\n",
      "Topic 12: 0.152*\"day\" + 0.134*\"content\" + 0.103*\"version\" + 0.045*\"image\" + 0.040*\"service\" + 0.029*\"management\" + 0.025*\"sign\" + 0.024*\"stop\" + 0.023*\"glitch\" + 0.020*\"kind\"\n",
      "Topic 13: 0.256*\"tweet\" + 0.087*\"option\" + 0.071*\"information\" + 0.067*\"notification\" + 0.066*\"page\" + 0.032*\"community\" + 0.022*\"tick\" + 0.022*\"topic\" + 0.021*\"hour\" + 0.019*\"link\"\n",
      "Topic 14: 0.762*\"\" + 0.012*\"fire\" + 0.010*\"watch\" + 0.009*\"hope\" + 0.007*\"level\" + 0.006*\"none\" + 0.004*\"kid\" + 0.004*\"fleet\" + 0.004*\"dollar\" + 0.003*\"item\"\n",
      "Topic 15: 0.179*\"speech\" + 0.114*\"user\" + 0.095*\"musk\" + 0.068*\"freedom\" + 0.068*\"place\" + 0.045*\"opinion\" + 0.037*\"people\" + 0.022*\"alot\" + 0.016*\"rating\" + 0.016*\"thought\"\n",
      "Topic 16: 0.517*\"video\" + 0.205*\"player\" + 0.080*\"update\" + 0.016*\"suck\" + 0.012*\"auto\" + 0.011*\"text\" + 0.010*\"verification\" + 0.008*\"hell\" + 0.008*\"ownership\" + 0.007*\"order\"\n",
      "Topic 17: 0.181*\"way\" + 0.147*\"lot\" + 0.054*\"stuff\" + 0.051*\"people\" + 0.039*\"thread\" + 0.031*\"business\" + 0.030*\"tiktok\" + 0.027*\"cat\" + 0.024*\"thing\" + 0.023*\"info\"\n",
      "Topic 18: 0.348*\"twitter\" + 0.133*\"people\" + 0.064*\"news\" + 0.057*\"application\" + 0.047*\"feature\" + 0.041*\"month\" + 0.033*\"bot\" + 0.024*\"company\" + 0.023*\"world\" + 0.014*\"review\"\n",
      "Topic 19: 0.364*\"account\" + 0.178*\"update\" + 0.037*\"job\" + 0.036*\"email\" + 0.035*\"reason\" + 0.023*\"password\" + 0.017*\"function\" + 0.016*\"login\" + 0.014*\"access\" + 0.014*\"moment\"\n"
     ]
    }
   ],
   "source": [
    "num_topics=20\n",
    "lda_model = run_lda(filtered_data, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of topics\n",
    "topics = lda_model.print_topics(15,4)\n",
    "\n",
    "# Get the topic-word distribution for a specific topic\n",
    "topic_words = lda_model.get_topic_terms(0, 5)\n",
    "\n",
    "# Get the document-topic distribution for a specific document\n",
    "doc = \"This is a sample document.\"\n",
    "doc_bow = lda_model.id2word.doc2bow(doc.split())\n",
    "doc_topic_distribution = lda_model.get_document_topics(doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.148*\"medium\" + 0.117*\"thing\" + 0.098*\"change\" + 0.037*\"life\"'),\n",
       " (7, '0.206*\"problem\" + 0.123*\"year\" + 0.070*\"porn\" + 0.062*\"rule\"'),\n",
       " (8, '0.125*\"feed\" + 0.116*\"apps\" + 0.078*\"ad\" + 0.073*\"internet\"'),\n",
       " (12, '0.152*\"day\" + 0.134*\"content\" + 0.103*\"version\" + 0.045*\"image\"'),\n",
       " (19, '0.364*\"account\" + 0.178*\"update\" + 0.037*\"job\" + 0.036*\"email\"'),\n",
       " (16, '0.517*\"video\" + 0.205*\"player\" + 0.080*\"update\" + 0.016*\"suck\"'),\n",
       " (13,\n",
       "  '0.256*\"tweet\" + 0.087*\"option\" + 0.071*\"information\" + 0.067*\"notification\"'),\n",
       " (18, '0.348*\"twitter\" + 0.133*\"people\" + 0.064*\"news\" + 0.057*\"application\"'),\n",
       " (1, '0.133*\"post\" + 0.105*\"bug\" + 0.058*\"one\" + 0.054*\"fix\"'),\n",
       " (14, '0.762*\"\" + 0.012*\"fire\" + 0.010*\"watch\" + 0.009*\"hope\"'),\n",
       " (15, '0.179*\"speech\" + 0.114*\"user\" + 0.095*\"musk\" + 0.068*\"freedom\"'),\n",
       " (3, '0.126*\"star\" + 0.094*\"phone\" + 0.082*\"button\" + 0.073*\"timeline\"'),\n",
       " (10, '0.220*\"platform\" + 0.062*\"garbage\" + 0.052*\"team\" + 0.052*\"medium\"'),\n",
       " (6,\n",
       "  '0.222*\"experience\" + 0.126*\"love\" + 0.059*\"message\" + 0.049*\"improvement\"'),\n",
       " (11, '0.115*\"work\" + 0.084*\"reason\" + 0.068*\"week\" + 0.054*\"reply\"')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_id = 0  # Replace with the desired topic ID\n",
    "topic_words = lda_model.get_topic_terms(topic_id, topn=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "filtered_words = [word for word, prob in topic_words if prob > threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Coherence: 0\n",
      "Topic: 0.148*\"medium\" + 0.117*\"thing\" + 0.098*\"change\" + 0.037*\"life\"\n",
      "\n",
      "Topic Coherence: 7\n",
      "Topic: 0.206*\"problem\" + 0.123*\"year\" + 0.070*\"porn\" + 0.062*\"rule\"\n",
      "\n",
      "Topic Coherence: 8\n",
      "Topic: 0.125*\"feed\" + 0.116*\"apps\" + 0.078*\"ad\" + 0.073*\"internet\"\n",
      "\n",
      "Topic Coherence: 12\n",
      "Topic: 0.152*\"day\" + 0.134*\"content\" + 0.103*\"version\" + 0.045*\"image\"\n",
      "\n",
      "Topic Coherence: 19\n",
      "Topic: 0.364*\"account\" + 0.178*\"update\" + 0.037*\"job\" + 0.036*\"email\"\n",
      "\n",
      "Topic Coherence: 16\n",
      "Topic: 0.517*\"video\" + 0.205*\"player\" + 0.080*\"update\" + 0.016*\"suck\"\n",
      "\n",
      "Topic Coherence: 13\n",
      "Topic: 0.256*\"tweet\" + 0.087*\"option\" + 0.071*\"information\" + 0.067*\"notification\"\n",
      "\n",
      "Topic Coherence: 18\n",
      "Topic: 0.348*\"twitter\" + 0.133*\"people\" + 0.064*\"news\" + 0.057*\"application\"\n",
      "\n",
      "Topic Coherence: 1\n",
      "Topic: 0.133*\"post\" + 0.105*\"bug\" + 0.058*\"one\" + 0.054*\"fix\"\n",
      "\n",
      "Topic Coherence: 14\n",
      "Topic: 0.762*\"\" + 0.012*\"fire\" + 0.010*\"watch\" + 0.009*\"hope\"\n",
      "\n",
      "Topic Coherence: 15\n",
      "Topic: 0.179*\"speech\" + 0.114*\"user\" + 0.095*\"musk\" + 0.068*\"freedom\"\n",
      "\n",
      "Topic Coherence: 3\n",
      "Topic: 0.126*\"star\" + 0.094*\"phone\" + 0.082*\"button\" + 0.073*\"timeline\"\n",
      "\n",
      "Topic Coherence: 10\n",
      "Topic: 0.220*\"platform\" + 0.062*\"garbage\" + 0.052*\"team\" + 0.052*\"medium\"\n",
      "\n",
      "Topic Coherence: 6\n",
      "Topic: 0.222*\"experience\" + 0.126*\"love\" + 0.059*\"message\" + 0.049*\"improvement\"\n",
      "\n",
      "Topic Coherence: 11\n",
      "Topic: 0.115*\"work\" + 0.084*\"reason\" + 0.068*\"week\" + 0.054*\"reply\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_coherence, topic in topics:\n",
    "    print(f\"Topic Coherence: {topic_coherence}\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m dictionary \u001b[39m=\u001b[39m Dictionary(filtered_data)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Convert the document to its bag-of-words representation\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m doc_bow \u001b[39m=\u001b[39m dictionary\u001b[39m.\u001b[39mdoc2bow(document)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Get the topic distribution for the document\u001b[39;00m\n\u001b[0;32m     13\u001b[0m topic_distribution \u001b[39m=\u001b[39m lda_model\u001b[39m.\u001b[39mget_document_topics(doc_bow)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Assuming you have trained an LDA model called 'lda_model'\n",
    "# and you have a document represented as a bag-of-words 'doc_bow'\n",
    "\n",
    "# Create a dictionary mapping words to their numerical ids\n",
    "dictionary = Dictionary(filtered_data)\n",
    "\n",
    "# Convert the document to its bag-of-words representation\n",
    "doc_bow = dictionary.doc2bow(document)\n",
    "\n",
    "# Get the topic distribution for the document\n",
    "topic_distribution = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "# Iterate over the topics and their probabilities\n",
    "for topic, prob in topic_distribution:\n",
    "    print(f\"Topic: {topic}, Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0, Probability: 0.016666842624545097\n",
      "Topic: 1, Probability: 0.3499999940395355\n",
      "Topic: 2, Probability: 0.016666842624545097\n",
      "Topic: 3, Probability: 0.016666842624545097\n",
      "Topic: 4, Probability: 0.016666842624545097\n",
      "Topic: 5, Probability: 0.016666842624545097\n",
      "Topic: 6, Probability: 0.016666842624545097\n",
      "Topic: 7, Probability: 0.016666842624545097\n",
      "Topic: 8, Probability: 0.016666842624545097\n",
      "Topic: 9, Probability: 0.016666842624545097\n",
      "Topic: 10, Probability: 0.016666842624545097\n",
      "Topic: 11, Probability: 0.016666842624545097\n",
      "Topic: 12, Probability: 0.016666842624545097\n",
      "Topic: 13, Probability: 0.3499968349933624\n",
      "Topic: 14, Probability: 0.016666842624545097\n",
      "Topic: 15, Probability: 0.016666842624545097\n",
      "Topic: 16, Probability: 0.016666842624545097\n",
      "Topic: 17, Probability: 0.016666842624545097\n",
      "Topic: 18, Probability: 0.016666842624545097\n",
      "Topic: 19, Probability: 0.016666842624545097\n"
     ]
    }
   ],
   "source": [
    "document=\"copy link bookmark buttons keep switching places reasons lmao\".split(\" \")\n",
    "doc_bow = dictionary.doc2bow(document)\n",
    "\n",
    "# Get the topic distribution for the document\n",
    "topic_distribution = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "# Iterate over the topics and their probabilities\n",
    "for topic, prob in topic_distribution:\n",
    "    print(f\"Topic: {topic}, Probability: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f98acb1971848e61526fd913f9722ee15e73d88477824fe14df80a35eeb01c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
